defaults:
  - _self_
  - model: unet_small
  - task: denoise
  - noise_scheduler: linear_1k
  - dataset: cifar10
  - fold: cifar10_fold
  - optimizer: adamw
  - logger: wandb
  - lr_scheduler: cosine_with_warmup

compile_model: false
load_from_ckpt_path: null

# model checkpoint callback
model_checkpoint:
  monitor: "val_loss"
  mode: "min"
  save_top_k: 10


# Dataloaders
train_dataloader_kwargs:
  batch_size: 128
  shuffle: true
  num_workers: 2
  pin_memory: false
  persistent_workers: true

val_dataloader_kwargs:
  batch_size: 32
  shuffle: false
  num_workers: 2
#  pin_memory: true
  pin_memory: false
  persistent_workers: true


# Trainer
trainer_kwargs:
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  # precision: "bf16-mixed" # weights in float32, activations in bfloat16
  benchmark: true
  log_every_n_steps: 20
  max_epochs: 300
  val_check_interval: null
  num_sanity_val_steps: 1
  accelerator: "gpu"
  devices: -1
  detect_anomaly: true
