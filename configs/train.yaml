defaults:
  - _self_
  - model: unet_ddpm  
  - augmentation: noop
  - early_stopping: normal
  - scheduler: none
  - optimizer: adamw
  - task: denoise
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe

val_check_interval: 20000                # check validation every `val_check_interval`` steps
batch_size: 128                             # final batch size.
apparent_batch_size: 128                    # batch size during training
num_workers: 1                            # number of workers for data loading
dry_logger: true                          # if true, will not log to wandb
exp_name: "ddpm_reproduce"                  # name of the experiment    
group_name: "first_run"  # maybe this would be useful for grouping experiments when we do hyperparameter search
max_epochs: 2500
max_steps: -1
early_stopping_metric: "val_loss"

loss:
  - type: L1Loss
    weight: 1.0
  
train_pct: 0.8   # Percentage of the dataset to use for training

noise_scheduler:
  kwargs:
    num_diffusion_timesteps: 1000     # Number of diffusion steps
    noise_scheduler: "linear"

hydra:
  sweeper:
    sampler:
      seed: 1337
    direction: maximize
    storage: null
    n_trials: 100
    n_jobs: 1
    params:
      dataset.kwargs.p_random_3d_rotation: range(0.0, 1.0, 0.1)